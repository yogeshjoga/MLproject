# -*- coding: utf-8 -*-
"""new 15th.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wXDkH5zz-6EgR4pPsByZOz3EHfuxJzCm
"""

# Importing Librarys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LassoCV
from sklearn.model_selection import KFold
from sklearn.linear_model import RidgeCV, Ridge

# Load the Excel file
file_path = '/content/CleanedDataBook.xlsx'
xls = pd.ExcelFile(file_path)

# Get output file path
#output_file_path = '/content/NewOutputDATA.xlsx'

# Load the Excel file
file_path = '/content/CleanedDataBook.xlsx'
xls = pd.ExcelFile(file_path)

# Dictionary to store PCA results
pca_results = {}

# Iterate over each sheet in the Excel file
for sheet_name in xls.sheet_names:
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name)

        # Check if the necessary columns 'log2(fold change)' exist
        if 'log2(fold change)' not in df.columns:
            print(f"'log2(fold change)' column not found in sheet: {sheet_name}")
            continue

        # Create a synthetic feature by adding some noise to the existing feature
        df['synthetic_feature'] = df['log2(fold change)'] + np.random.normal(0, 1, len(df))

        # Extract the relevant columns for PCA
        data = df[['log2(fold change)', 'synthetic_feature']]

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Perform PCA
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(scaled_data)

        # Create a DataFrame with the PCA results
        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        pca_df['Gene.symbol'] = df['Gene.symbol']

        # Store the results in the dictionary
        pca_results[sheet_name] = pca_df

    except Exception as e:
        print(f"An error occurred while processing sheet: {sheet_name}")
        print(str(e))

# Check if there are any PCA results before attempting to save
if pca_results:
    # Save the PCA results to a new Excel workbook
    output_file_path = '/content/Output.xlsx'
    with pd.ExcelWriter(output_file_path) as writer:
        for sheet_name, pca_df in pca_results.items():
            if not pca_df.empty:
                pca_df.to_excel(writer, sheet_name=sheet_name, index=False)
else:
    print("No valid PCA results to save.")

# Load the Excel file
file_path = '/content/CleanedDataBook.xlsx'
xls = pd.ExcelFile(file_path)

# Dictionary to store PCA results
pca_results = {}

# Iterate over each sheet in the Excel file
for sheet_name in xls.sheet_names:
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name)

        # Print the columns to understand the structure of the sheet
        print(f"Columns in sheet '{sheet_name}': {df.columns.tolist()}")

        # Check if the necessary columns 'log2(fold change)' exist
        if 'log2(fold change)' not in df.columns:
            print(f"'log2(fold change)' column not found in sheet: {sheet_name}")
            continue

        # Create a synthetic feature by adding some noise to the existing feature
        df['synthetic_feature'] = df['log2(fold change)'] + np.random.normal(0, 1, len(df))

        # Extract the relevant columns for PCA
        data = df[['log2(fold change)', 'synthetic_feature']]

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Perform PCA
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(scaled_data)

        # Create a DataFrame with the PCA results
        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        pca_df['Gene.symbol'] = df['Gene.symbol']

        # Store the results in the dictionary
        pca_results[sheet_name] = pca_df

    except Exception as e:
        print(f"An error occurred while processing sheet: {sheet_name}")
        print(str(e))

# Check if there are any PCA results before attempting to save
if pca_results:
    # Save the PCA results to a new Excel workbook
    output_file_path = '/content/Output.xlsx'
    with pd.ExcelWriter(output_file_path) as writer:
        for sheet_name, pca_df in pca_results.items():
            if not pca_df.empty:
                pca_df.to_excel(writer, sheet_name=sheet_name, index=False)
else:
    print("No valid PCA results to save.")

# Load the Excel file
file_path = '/content/CleanedDataBook.xlsx'
xls = pd.ExcelFile(file_path)

# Dictionary to store PCA results
pca_results = {}

# Iterate over each sheet in the Excel file
for sheet_name in xls.sheet_names:
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name)

        # Print the columns to understand the structure of the sheet
        print(f"Columns in sheet '{sheet_name}': {df.columns.tolist()}")

        # Check if the necessary columns 'x axis' and 'y axis' exist
        if 'x axis' not in df.columns or 'y axis' not in df.columns:
            print(f"'x axis' or 'y axis' column not found in sheet: {sheet_name}")
            continue

        # Create a synthetic feature by adding some noise to the existing feature
        df['synthetic_feature'] = df['x axis'] + np.random.normal(0, 1, len(df))

        # Extract the relevant columns for PCA
        data = df[['x axis', 'y axis', 'synthetic_feature']]

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)

        # Perform PCA
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(scaled_data)

        # Create a DataFrame with the PCA results
        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        pca_df['Gene.symbol'] = df['Gene.symbol']

        # Store the results in the dictionary
        pca_results[sheet_name] = pca_df

    except Exception as e:
        print(f"An error occurred while processing sheet: {sheet_name}")
        print(str(e))

# Check if there are any PCA results before attempting to save
if pca_results:
    # Save the PCA results to a new Excel workbook
    output_file_path = '/content/Output.xlsx'
    with pd.ExcelWriter(output_file_path) as writer:
        for sheet_name, pca_df in pca_results.items():
            if not pca_df.empty:
                pca_df.to_excel(writer, sheet_name=sheet_name, index=False)
else:
    print("No valid PCA results to save.")

# Load the Excel file
file_path = '/content/CleanedDataBook.xlsx'
xls = pd.ExcelFile(file_path)

# Dictionary to store PCA results
pca_results = {}

# Iterate over each sheet in the Excel file
for sheet_name in xls.sheet_names:
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name)

        # Print the columns to understand the structure of the sheet
        print(f"Columns in sheet '{sheet_name}': {df.columns.tolist()}")

        # Check if the necessary columns 'x axis' and 'y axis' exist
        if 'x axis' not in df.columns or 'y axis' not in df.columns:
            print(f"'x axis' or 'y axis' column not found in sheet: {sheet_name}")
            continue

        # Create a synthetic feature by adding some noise to the existing feature
        df['synthetic_feature'] = df['x axis'] + np.random.normal(0, 1, len(df))

        # Extract the relevant columns for PCA
        data = df[['x axis', 'y axis', 'synthetic_feature']]

        # Impute missing values with the mean
        imputer = SimpleImputer(strategy='mean')
        data_imputed = imputer.fit_transform(data)

        # Standardize the data
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data_imputed)

        # Perform PCA
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(scaled_data)

        # Create a DataFrame with the PCA results
        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        pca_df['Gene.symbol'] = df['Gene.symbol']

        # Store the results in the dictionary
        pca_results[sheet_name] = pca_df

    except Exception as e:
        print(f"An error occurred while processing sheet: {sheet_name}")
        print(str(e))

# Check if there are any PCA results before attempting to save
if pca_results:
    # Save the PCA results to a new Excel workbook
    output_file_path = '/content/Output.xlsx'
    with pd.ExcelWriter(output_file_path) as writer:
        for sheet_name, pca_df in pca_results.items():
            if not pca_df.empty:
                pca_df.to_excel(writer, sheet_name=sheet_name, index=False)
else:
    print("No valid PCA results to save.")

# Load the Excel file
xls = pd.ExcelFile(output_file_path)

# List to store all PCA results
combined_pca_df = pd.DataFrame()

# Iterate over each sheet in the Excel file
for sheet_name in xls.sheet_names:
    df = pd.read_excel(xls, sheet_name=sheet_name)

    # Check if the necessary columns 'PC1', 'PC2', and 'Gene.symbol' exist
    if 'PC1' not in df.columns or 'PC2' not in df.columns or 'Gene.symbol' not in df.columns:
        print(f"Necessary columns not found in sheet: {sheet_name}")
        continue

    # Add a column for the sheet name to distinguish between different datasets
    df['Dataset'] = sheet_name

    # Append to the combined DataFrame
    combined_pca_df = pd.concat([combined_pca_df, df[['PC1', 'PC2', 'Dataset']]], ignore_index=True)

# Limit the number of points plotted if the dataset is too large
max_points = 1000
if len(combined_pca_df) > max_points:
    combined_pca_df = combined_pca_df.sample(n=max_points, random_state=1)

# Define a custom dark color palette
dark_palette = sns.color_palette("dark", n_colors=combined_pca_df['Dataset'].nunique())

# Plot the combined PCA results with darker colors
plt.figure(figsize=(14, 10))
ax = sns.scatterplot(x='PC1', y='PC2', hue='Dataset', data=combined_pca_df, palette=dark_palette, alpha=1, s=100)

plt.title('Combined PCA of Gene Expression Data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Dataset', loc='best', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

# Combine all PCA results into a single DataFrame
combined_pca_df = pd.DataFrame()

for sheet_name in xls.sheet_names:
    df = pd.read_excel(xls, sheet_name=sheet_name)
    if 'PC1' in df.columns and 'PC2' in df.columns and 'Gene.symbol' in df.columns:
        df['Dataset'] = sheet_name
        combined_pca_df = pd.concat([combined_pca_df, df[['PC1', 'PC2', 'Dataset']]], ignore_index=True)

# Check for missing values
print(combined_pca_df.isnull().sum())

# Fill or drop missing values as needed
combined_pca_df.dropna(inplace=True)

# Create a synthetic target variable for the analysis
np.random.seed(42)
combined_pca_df['target'] = combined_pca_df['PC1'] * 0.5 + combined_pca_df['PC2'] * 0.3 + np.random.normal(size=len(combined_pca_df))

# Features and target variable
X = combined_pca_df[['PC1', 'PC2']]
y = combined_pca_df['target']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Perform LASSO regression with cross-validation
lasso_cv = LassoCV(alphas=np.logspace(-10, 2, 100), cv=5, random_state=42)
lasso_cv.fit(X_train, y_train)

# Optimal alpha
optimal_alpha = lasso_cv.alpha_
print(f"Optimal alpha: {optimal_alpha}")

# Coefficients
lasso_coefs = lasso_cv.coef_
print(f"LASSO coefficients: {lasso_coefs}")

# Mean square error for each alpha
mse_path = lasso_cv.mse_path_.mean(axis=1)

# Plot the mean square error for each alpha (log scale)
plt.figure(figsize=(14, 7))

# Plot A: Mean square error
plt.subplot(1, 2, 1)
plt.plot(np.log10(lasso_cv.alphas_), mse_path, 'r.-')
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Mean Square Error')
plt.title('Mean Square Error vs. Log(Alpha)')
plt.legend()
plt.grid(True)

# Plot B: Coefficients as a function of the regularization
plt.subplot(1, 2, 2)
plt.plot(np.log10(lasso_cv.alphas_), lasso_cv.path(X_train, y_train)[1].T)
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Coefficients')
plt.title('LASSO Coefficients vs. Log(Alpha)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Check for missing values
print(combined_pca_df.isnull().sum())

# Fill or drop missing values as needed
combined_pca_df.dropna(inplace=True)

# Create a synthetic target variable for the analysis
np.random.seed(42)
combined_pca_df['target'] = combined_pca_df['PC1'] * 0.5 + combined_pca_df['PC2'] * 0.3 + np.random.normal(size=len(combined_pca_df))

# Features and target variable
X = combined_pca_df[['PC1', 'PC2']]
y = combined_pca_df['target']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Perform LASSO regression with cross-validation
lasso_cv = LassoCV(alphas=np.logspace(-10, 2, 100), cv=5, random_state=42)
lasso_cv.fit(X_train, y_train)

# Optimal alpha
optimal_alpha = lasso_cv.alpha_
print(f"Optimal alpha: {optimal_alpha}")

# Coefficients
lasso_coefs = lasso_cv.coef_
print(f"LASSO coefficients: {lasso_coefs}")

# Mean square error for each alpha
mse_path = lasso_cv.mse_path_.mean(axis=1)

# Plot the mean square error for each alpha (log scale)
plt.figure(figsize=(14, 7))

# Plot A: Mean square error
plt.subplot(1, 2, 1)
plt.plot(np.log10(lasso_cv.alphas_), mse_path, 'r.-')
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Mean Square Error')
plt.title('Mean Square Error vs. Log(Alpha)')
plt.legend()
plt.grid(True)

# Plot B: Coefficients as a function of the regularization
plt.subplot(1, 2, 2)
plt.plot(np.log10(lasso_cv.alphas_), lasso_cv.path(X_train, y_train)[1].T)
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Coefficients')
plt.title('LASSO Coefficients vs. Log(Alpha)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Plot the Mean Squared Error for each alpha to visualize misclassification error
plt.figure(figsize=(10, 6))
plt.errorbar(np.log10(lasso_cv.alphas_), mse_path, yerr=lasso_cv.mse_path_.std(axis=1), fmt='o', color='red', ecolor='lightgray', elinewidth=3, capsize=0)
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='black', label='Optimal Alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Mean Squared Error')
plt.title('Misclassification Error Plot (Mean Squared Error)')
plt.legend()
plt.grid(True)
plt.show()

# Perform Ridge regression with cross-validation
alphas = np.logspace(-10, 2, 100)
ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)
ridge_cv.fit(X_train, y_train)

# Optimal alpha
optimal_alpha = ridge_cv.alpha_
print(f"Optimal alpha: {optimal_alpha}")

# Coefficients
ridge_coefs = ridge_cv.coef_
print(f"Ridge coefficients: {ridge_coefs}")

# Mean square error for each alpha
mse_path = ridge_cv.cv_values_.mean(axis=0)

# Plot the mean square error for each alpha (log scale)
plt.figure(figsize=(14, 7))

# Plot A: Mean square error
plt.subplot(1, 2, 1)
plt.plot(np.log10(alphas), mse_path, 'r.-')
plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Mean Square Error')
plt.title('Mean Square Error vs. Log(Alpha)')
plt.legend()
plt.grid(True)

# Plot B: Coefficients as a function of the regularization
plt.subplot(1, 2, 2)
ridge_path_coefs = []
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train, y_train)
    ridge_path_coefs.append(ridge.coef_)

ridge_path_coefs = np.array(ridge_path_coefs)

for coef in ridge_path_coefs.T:
    plt.plot(np.log10(alphas), coef)

plt.axvline(np.log10(optimal_alpha), linestyle='--', color='k', label='Optimal alpha')
plt.xlabel('Log10(Alpha)')
plt.ylabel('Coefficients')
plt.title('Ridge Coefficients vs. Log(Alpha)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()